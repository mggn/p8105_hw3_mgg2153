p8105\_hw3\_mgg2153
================
mggn
10/7/2020

## Problem 1, as discussed in class

``` r
data("instacart")
```

This dataset contains 1384617 rows and 15 columns.

Observations are the level of items in orders by user. There are user /
order variables – user ID, order ID, order day, and order hour. There
are also item variables – name, aisle, department, and some numeric
codes.

How many aisles, and which are most items from?

``` r
instacart %>% 
    count(aisle) %>% 
    arrange(desc(n))
```

    ## # A tibble: 134 x 2
    ##    aisle                              n
    ##    <chr>                          <int>
    ##  1 fresh vegetables              150609
    ##  2 fresh fruits                  150473
    ##  3 packaged vegetables fruits     78493
    ##  4 yogurt                         55240
    ##  5 packaged cheese                41699
    ##  6 water seltzer sparkling water  36617
    ##  7 milk                           32644
    ##  8 chips pretzels                 31269
    ##  9 soy lactosefree                26240
    ## 10 bread                          23635
    ## # ... with 124 more rows

Let’s make a plot

``` r
instacart %>% 
    count(aisle) %>% 
    filter(n > 10000) %>% 
    mutate(
        aisle = factor(aisle),
        aisle = fct_reorder(aisle, n)
    ) %>% 
    ggplot(aes(x = aisle, y = n)) + 
    geom_point() + 
    theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1))
```

<img src="p8105_hw3_mgg2153_files/figure-gfm/unnamed-chunk-3-1.png" width="90%" />

Let’s make a table\!\!

``` r
instacart %>% 
    filter(aisle %in% c("baking ingredients", "dog food care", "packaged vegetables fruits")) %>% 
    group_by(aisle) %>% 
    count(product_name) %>% 
    mutate(rank = min_rank(desc(n))) %>% 
    filter(rank < 4) %>% 
    arrange(aisle, rank) %>% 
    knitr::kable()
```

| aisle                      | product\_name                                 |    n | rank |
| :------------------------- | :-------------------------------------------- | ---: | ---: |
| baking ingredients         | Light Brown Sugar                             |  499 |    1 |
| baking ingredients         | Pure Baking Soda                              |  387 |    2 |
| baking ingredients         | Cane Sugar                                    |  336 |    3 |
| dog food care              | Snack Sticks Chicken & Rice Recipe Dog Treats |   30 |    1 |
| dog food care              | Organix Chicken & Brown Rice Recipe           |   28 |    2 |
| dog food care              | Small Dog Biscuits                            |   26 |    3 |
| packaged vegetables fruits | Organic Baby Spinach                          | 9784 |    1 |
| packaged vegetables fruits | Organic Raspberries                           | 5546 |    2 |
| packaged vegetables fruits | Organic Blueberries                           | 4966 |    3 |

Apples vs ice cream..

``` r
instacart %>% 
    filter(product_name %in% c("Pink Lady Apples", "Coffee Ice Cream")) %>% 
    group_by(product_name, order_dow) %>% 
    summarize(mean_hour = mean(order_hour_of_day)) %>% 
    pivot_wider(
        names_from = order_dow,
        values_from = mean_hour
    )
```

    ## `summarise()` regrouping output by 'product_name' (override with `.groups` argument)

    ## # A tibble: 2 x 8
    ## # Groups:   product_name [2]
    ##   product_name       `0`   `1`   `2`   `3`   `4`   `5`   `6`
    ##   <chr>            <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>
    ## 1 Coffee Ice Cream  13.8  14.3  15.4  15.3  15.2  12.3  13.8
    ## 2 Pink Lady Apples  13.4  11.4  11.7  14.2  11.6  12.8  11.9

### `summarise()` regrouping output by ‘product\_name’ (override with `.groups` argument)

## Problem 2

Here we are uploading the accelerometers data and tidying it just a bit,
including  
(1) cleaning names (2) changing variable types (3) creating a binary
weekday variable

``` r
accel_df = read_csv("./data/accel_data.csv") %>%
  janitor::clean_names() %>%
  mutate(
    day = as.factor(day),
    week = as.factor(week),
    weekday = if_else(day %in% c("Saturday", "Sunday"), "FALSE", "TRUE")
  ) %>%
  relocate(weekday) %>%
  pivot_longer(
    activity_1:activity_1440,
    names_to = "minute",
    names_prefix = "activity_",
    values_to = "acceleration_force"
  ) %>%
  mutate(
    minute = as.factor(minute),
    acceleration_force = as.numeric(acceleration_force)
  )
```

    ## Parsed with column specification:
    ## cols(
    ##   .default = col_double(),
    ##   day = col_character()
    ## )

    ## See spec(...) for full column specifications.

The new, tidier dataset, accel\_df has 50400observations and 6
variables. Those variables are weekday, week, day\_id, day, minute,
acceleration\_force

``` r
accel_df %>%
group_by(day, week) %>%
summarize(activity_total = sum(acceleration_force)) %>%
pivot_wider(
names_from = day,
values_from = activity_total
)
```

    ## `summarise()` regrouping output by 'day' (override with `.groups` argument)

    ## # A tibble: 5 x 8
    ##   week   Friday  Monday Saturday Sunday Thursday Tuesday Wednesday
    ##   <fct>   <dbl>   <dbl>    <dbl>  <dbl>    <dbl>   <dbl>     <dbl>
    ## 1 1     480543.  78828.   376254 631105  355924. 307094.   340115.
    ## 2 2     568839  295431    607175 422018  474048  423245    440962 
    ## 3 3     467420  685910    382928 467052  371230  381507    468869 
    ## 4 4     154049  409450      1440 260617  340291  319568    434460 
    ## 5 5     620860  389080      1440 138421  549658  367824    445366

``` r
#Accelerometer data allows the inspection activity over the course of the day. Make a single-panel plot that shows the 24-hour activity time courses for each day and #use color to indicate day of the week. Describe in words any patterns or conclusions you can make based on this graph.

accel_df %>%
  filter(week == 1) %>%
  mutate(
    day = forcats::fct_relevel(day, c("Monday", "Tuesday", "Wednesday", "Thursday", "Friday", "Saturday", "Sunday")),
    #lubridate::as.duration() lemme try to figure this one out
    ) %>%
  ggplot(aes(x = minute, y = acceleration_force, fill = day))+
  geom_point(aes(color = day))+
  facet_grid(. ~ day)+
  theme(legend.position = "none")
```

<img src="p8105_hw3_mgg2153_files/figure-gfm/unnamed-chunk-6-1.png" width="90%" />

## Problem 3

By the way, this blog post was helpful in figuring out what tf tenths of
measurements meant:
<http://annahydro.blogspot.com/2016/09/making-sense-of-noaa-precipitation-data.html>

gotta give proper credit to the people that investigated before me.
Thanks google\!

``` r
data("ny_noaa")

ny_weather_df =
ny_noaa %>%
  janitor::clean_names() %>%
  mutate(
    tmin = as.numeric(tmin),
    tmax = as.numeric(tmax),
  ) %>%
  separate(date, into = c("year", "month", "day")) %>%
  mutate(
    month = month.abb[as.factor(month)],
    year = as.factor(year),
    day = as.factor(day),
    tmin = tmin/10,
    tmax = tmax/10,
    prcp = prcp/10
  )
```

Let’s see what’s going on with snowfall:

``` r
ny_weather_df %>%
  count(month, year, snow)%>%
  group_by(month, year)
```

    ## # A tibble: 14,722 x 4
    ## # Groups:   month, year [360]
    ##    month year   snow     n
    ##    <chr> <fct> <int> <int>
    ##  1 Apr   1981      0  6964
    ##  2 Apr   1981      3     8
    ##  3 Apr   1981      5     8
    ##  4 Apr   1981      8     5
    ##  5 Apr   1981     10     5
    ##  6 Apr   1981     13    19
    ##  7 Apr   1981     15     1
    ##  8 Apr   1981     20     2
    ##  9 Apr   1981     23     2
    ## 10 Apr   1981     25    14
    ## # ... with 14,712 more rows

Using this sort of crude method, it appears that the most common
frequent value for snowfall in mm for nyc is 0. This actually makes
sense because how many months of the year is it actually cold enough to
snow? Not that many (although with climate change who knows).

And now I shall attempt to make a two-panel plot, which I ***believe***
you need the library patchwork for:

``` r
library(wesanderson)


ny_weather_df %>%
  filter(month %in% c("Jan", "Jul")) %>%
  group_by(id, year, month)%>%
  summarize(
    mean_tmax = mean(tmax, na.rm = TRUE)
  ) %>%
  ggplot(aes(x = year, y = mean_tmax, group = id, color = id))+
  geom_point(alpha = .5)+
  geom_path()+
  facet_grid(~month)+
  labs(
    title = "Average max temperature in January and July in each station across years",
    x = "Year",
    y = "Average maximum temperature",
    caption = "An attempt: Homework 3 ny_noaa edition"
  )+ theme_bw()+
  scale_color_hue (name = "id", h = c(100, 300))+
  scale_x_discrete(
    breaks = c(1981, 1985, 1990, 1995, 2000, 2005, 2010 )
  )+
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1), legend.position = "none")
```

    ## `summarise()` regrouping output by 'id', 'year' (override with `.groups` argument)

    ## Warning: Removed 5970 rows containing missing values (geom_point).

    ## Warning: Removed 5931 row(s) containing missing values (geom_path).

<img src="p8105_hw3_mgg2153_files/figure-gfm/unnamed-chunk-8-1.png" width="90%" />

``` r
library(patchwork)
library(ggridges)

#Make a two-panel plot showing (i) tmax vs tmin for the full dataset (note that a scatterplot may not be the best option); and (ii) make a plot showing the distribution of snowfall values greater than 0 and less than 100 separately by year.
#violin plot
#box plots????

tmax_v_tmin = 
ny_weather_df %>%
  select(tmax, tmin) %>%
  drop_na(tmax, tmin) %>%
  ggplot(aes(x = tmax, y = tmin))+
  geom_hex()+
  theme(legend.position = "none")

snow_dist_1 = 
ny_weather_df %>%
  filter(year %in% c(1981:1990)) %>%
  drop_na(snow) %>%
  filter(snow <= 100) %>%
  group_by(id, year, month) %>%
  ggplot(aes(x = year, y = snow, fill = year))+
  geom_violin()+
  theme(axis.text.x = element_text(angle = 90), legend.position = "none")

snow_dist_2 =
 ny_weather_df %>%
  filter(year %in% c(1991:2000)) %>%
  drop_na(snow) %>%
  filter(snow <= 100) %>%
  group_by(id, year, month) %>%
  ggplot(aes(x = year, y = snow, fill = year))+
  geom_violin()+
  theme(axis.text.x = element_text(angle = 90), legend.position = "none") 

snow_dist_3 =
  ny_weather_df %>%
  filter(year %in% c(2001:2010)) %>%
  drop_na(snow) %>%
  filter(snow <= 100) %>%
  group_by(id, year, month) %>%
  ggplot(aes(x = year, y = snow, fill = year))+
  geom_violin()+
  theme(axis.text.x = element_text(angle = 90), legend.position = "none")

(snow_dist_1 + snow_dist_2)/(snow_dist_3 + tmax_v_tmin)
```

<img src="p8105_hw3_mgg2153_files/figure-gfm/unnamed-chunk-9-1.png" width="90%" />
