---
title: "p8105_hw3_mgg2153"
author: "mggn"
date: "10/7/2020"
output: github_document
---

## Problem 1, as discussed in class

```{r setup, include=FALSE}
library(tidyverse)
library(p8105.datasets)

knitr::opts_chunk$set(
	fig.width = 6, 
  fig.asp = .6,
  out.width = "90%"
)
theme_set(theme_minimal() + theme(legend.position = "bottom"))
options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis"
)
scale_colour_discrete = scale_colour_viridis_d
scale_fill_discrete = scale_fill_viridis_d
```

```{r}
data("instacart")
```

This dataset contains `r nrow(instacart)` rows and ... columns. 

Observations are the level of items in orders by user. There are user / order variables -- user ID, order ID, order day, and order hour. There are also item variables -- name, aisle, department, and some numeric codes. 

How many aisles, and which are most items from?

```{r}
instacart %>% 
	count(aisle) %>% 
	arrange(desc(n))
```


Let's make a plot

```{r}
instacart %>% 
	count(aisle) %>% 
	filter(n > 10000) %>% 
	mutate(
		aisle = factor(aisle),
		aisle = fct_reorder(aisle, n)
	) %>% 
	ggplot(aes(x = aisle, y = n)) + 
	geom_point() + 
	theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1))
```


Let's make a table!!

```{r}
instacart %>% 
	filter(aisle %in% c("baking ingredients", "dog food care", "packaged vegetables fruits")) %>% 
	group_by(aisle) %>% 
	count(product_name) %>% 
	mutate(rank = min_rank(desc(n))) %>% 
	filter(rank < 4) %>% 
	arrange(aisle, rank) %>% 
	knitr::kable()
```


Apples vs ice cream..

```{r}
instacart %>% 
	filter(product_name %in% c("Pink Lady Apples", "Coffee Ice Cream")) %>% 
	group_by(product_name, order_dow) %>% 
	summarize(mean_hour = mean(order_hour_of_day)) %>% 
	pivot_wider(
		names_from = order_dow,
		values_from = mean_hour
	)
```


### `summarise()` regrouping output by 'product_name' (override with `.groups` argument)

## Problem 2

Here we are uploading the accelerometers data and tidying it just a bit, including  
(1) cleaning names  (2) changing variable types  (3) creating a binary weekday variable

```{r accel_data}
accel_df = read_csv("./data/accel_data.csv") %>%
  janitor::clean_names() %>%
  mutate(
    day = as.factor(day),
    week = as.factor(week),
    weekday = if_else(day %in% c("Saturday", "Sunday"), "FALSE", "TRUE")
  ) %>%
  relocate(weekday) %>%
  pivot_longer(
    activity_1:activity_1440,
    names_to = "minute",
    names_prefix = "activity_",
    values_to = "acceleration_force"
  ) %>%
  mutate(
    minute = as.factor(minute),
    acceleration_force = as.numeric(acceleration_force)
  )
```


```{r two_parttwo}
accel_df %>%
group_by(day, week) %>%
summarize(activity_total = sum(acceleration_force)) %>%
pivot_wider(
names_from = day,
values_from = activity_total
)
```


## Problem 3

By the way, this blog post was helpful in figuring out what tf tenths of measurements
meant: http://annahydro.blogspot.com/2016/09/making-sense-of-noaa-precipitation-data.html

gotta give proper credit to the people that investigated before me. Thanks google!

```{r load_ny_noaa}

data("ny_noaa")

ny_weather_df =
ny_noaa %>%
  janitor::clean_names() %>%
  mutate(
    tmin = as.numeric(tmin),
    tmax = as.numeric(tmax),
  ) %>%
  separate(date, into = c("year", "month", "day")) %>%
  mutate(
    month = month.abb[as.factor(month)],
    year = as.factor(year),
    day = as.factor(day),
    tmin = tmin/10,
    tmax = tmax/10,
    prcp = prcp/10
  )


```

Let's see what's going on with snowfall:
```{r}
ny_weather_df %>%
  count(month, year, snow)%>%
  group_by(month, year)
```

Using this sort of crude method, it appears that the most common frequent value for
snowfall in mm for nyc is 0. This actually makes sense because how many months of the year
is it actually cold enough to snow? Not that many (although with climate change who knows).

And now I shall attempt to make a two-panel plot, which I ***believe*** you need
the library patchwork for:

```{r}
library(wesanderson)


ny_weather_df %>%
  filter(month %in% c("Jan", "Jul")) %>%
  group_by(id, year, month)%>%
  summarize(
    mean_tmax = mean(tmax, na.rm = TRUE)
  ) %>%
  ggplot(aes(x = year, y = mean_tmax, group = id, color = id))+
  geom_point(alpha = .5)+
  geom_path()+
  facet_grid(~month)+
  labs(
    title = "Average max temperature in January and July in each station across years",
    x = "Year",
    y = "Average maximum temperature",
    caption = "An attempt: Homework 3 ny_noaa edition"
  )+ theme_bw()+
  scale_color_hue (name = "id", h = c(100, 300))+
  scale_x_discrete(
    breaks = c(1981, 1985, 1990, 1995, 2000, 2005, 2010 )
  )+
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1), legend.position = "none")
  


```


```{r}
library(patchwork)
library(ggridges)

#Make a two-panel plot showing (i) tmax vs tmin for the full dataset (note that a scatterplot may not be the best option); and (ii) make a plot showing the distribution of snowfall values greater than 0 and less than 100 separately by year.
#violin plot
#box plots????

tmax_v_tmin = 
ny_weather_df %>%
  select(tmax, tmin) %>%
  drop_na(tmax, tmin) %>%
  ggplot(aes(x = tmax, y = tmin))+
  geom_hex()

snow_dist_1 = 
ny_weather_df %>%
  filter(year %in% c(1981:1990)) %>%
  drop_na(snow) %>%
  group_by(id, year, month) %>%
  ggplot(aes(x = year, y = snow))+
  geom_boxplot()+
  theme(axis.text.x = element_text(angle = 90))

snow_dist_2 =
 ny_weather_df %>%
  filter(year %in% c(1991:2000)) %>%
  drop_na(snow) %>%
  group_by(id, year, month) %>%
  ggplot(aes(x = year, y = snow))+
  geom_boxplot()+
  theme(axis.text.x = element_text(angle = 90)) 

snow_dist_3 =
  ny_weather_df %>%
  filter(year %in% c(2001:2010)) %>%
  drop_na(snow) %>%
  group_by(id, year, month) %>%
  ggplot(aes(x = year, y = snow))+
  geom_boxplot()+
  theme(axis.text.x = element_text(angle = 90))
  

```